{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detailed Explanation of Word2Vec and Implementation in PyTorch**\n",
    "---\n",
    "\n",
    "## **1. What is Word2Vec?**\n",
    "Word2Vec is a word embedding technique that transforms words into dense vectors while capturing their semantic relationships. It was introduced by **Tomas Mikolov et al. at Google** in 2013 and is widely used in natural language processing (NLP).\n",
    "\n",
    "Unlike **one-hot encoding**, where words are represented as sparse, independent vectors, **Word2Vec maps similar words to nearby points in a continuous vector space**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Word2Vec Works**\n",
    "Word2Vec is based on two primary architectures:\n",
    "\n",
    "1. **Continuous Bag of Words (CBOW)**  \n",
    "   - **Predicts a target word** based on surrounding context words.  \n",
    "   - Faster but may lose some word order information.  \n",
    "\n",
    "   **Example:**  \n",
    "   Given the context words `[\"The\", \"cat\", \"on\", \"the\", \"mat\"]`, predict the missing word `\"sat\"`.\n",
    "\n",
    "2. **Skip-Gram**  \n",
    "   - **Predicts context words** given a target word.  \n",
    "   - Slower but works better with rare words.\n",
    "\n",
    "   **Example:**  \n",
    "   Given `\"sat\"`, predict likely surrounding words: `[\"The\", \"cat\", \"on\", \"the\", \"mat\"]`.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Training Word2Vec**\n",
    "Training Word2Vec involves optimizing word vectors so that words appearing in similar contexts have similar vector representations. The model uses:\n",
    "- **Negative Sampling:** Instead of updating all weights, it updates a few random samples.\n",
    "- **Hierarchical Softmax:** Reduces computation by using a binary tree structure.\n",
    "\n",
    "After training, **similar words will have similar vector representations**, meaning words like `\"king\"` and `\"queen\"` will be closer in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Word2Vec in PyTorch (Step-by-Step Implementation)**\n",
    "\n",
    "### **Step 1: Install Dependencies**\n",
    "```bash\n",
    "pip install torch torchvision nltk\n",
    "```\n",
    "\n",
    "### **Step 2: Preprocess Text Data**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "text = \"The cat sat on the mat. The dog barked at the cat.\"\n",
    "\n",
    "# Tokenize the text\n",
    "nltk.download('punkt')\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = list(set(tokens))\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "# Convert words to indices\n",
    "data = [word2idx[word] for word in tokens]\n",
    "\n",
    "print(\"Vocabulary:\", word2idx)\n",
    "print(\"Encoded Data:\", data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Generate Training Data (Skip-Gram Model)**\n",
    "```python\n",
    "def generate_skipgram_data(data, window_size=2):\n",
    "    pairs = []\n",
    "    for center in range(len(data)):\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context = center + w\n",
    "            if context >= 0 and context < len(data) and context != center:\n",
    "                pairs.append((data[center], data[context]))\n",
    "    return pairs\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_skipgram_data(data, window_size)\n",
    "\n",
    "print(\"Example training pairs (word index):\", training_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Build the Word2Vec Model in PyTorch**\n",
    "```python\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, center_word):\n",
    "        embedded = self.embeddings(center_word)  # Get embeddings\n",
    "        output = self.linear(embedded)  # Predict context words\n",
    "        return output\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize model\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Train the Model**\n",
    "```python\n",
    "# Loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert data to tensors\n",
    "training_data_tensors = [(torch.tensor(center, dtype=torch.long),\n",
    "                          torch.tensor(context, dtype=torch.long)) for center, context in training_data]\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in training_data_tensors:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center.unsqueeze(0))  # Forward pass\n",
    "        loss = loss_function(output, context.unsqueeze(0))  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Extract Word Embeddings**\n",
    "```python\n",
    "word_vectors = model.embeddings.weight.data\n",
    "\n",
    "# Print embeddings for some words\n",
    "for word in [\"cat\", \"dog\", \"mat\"]:\n",
    "    print(f\"Embedding for '{word}': {word_vectors[word2idx[word]]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Word2Vec in Gensim (Easier Alternative)**\n",
    "If you donâ€™t want to train from scratch, use the `gensim` library.\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\"The cat sat on the mat.\", \"The dog barked at the cat.\"]\n",
    "tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=10, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Get word vectors\n",
    "print(w2v_model.wv[\"cat\"])  # Get vector for 'cat'\n",
    "print(w2v_model.wv.most_similar(\"cat\"))  # Find similar words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Advantages of Word2Vec**\n",
    "âœ… **Semantic Meaning**  \n",
    "   - Words that appear in similar contexts have similar vectors.  \n",
    "   - Example: `\"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"`  \n",
    "\n",
    "âœ… **Dimensionality Reduction**  \n",
    "   - Converts high-dimensional one-hot vectors into dense embeddings.\n",
    "\n",
    "âœ… **Works Well in NLP Tasks**  \n",
    "   - Improves performance in tasks like text classification and sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Disadvantages of Word2Vec**\n",
    "ðŸš« **Requires a Large Corpus**  \n",
    "   - Needs a lot of text to train meaningful embeddings.\n",
    "\n",
    "ðŸš« **Ignores Word Order**  \n",
    "   - Works on the assumption that context words are sufficient for meaning.\n",
    "\n",
    "ðŸš« **Out-of-Vocabulary (OOV) Words**  \n",
    "   - Cannot handle new words unless retrained.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Alternatives to Word2Vec**\n",
    "- **GloVe (Global Vectors for Word Representation)**  \n",
    "  - Uses matrix factorization to capture co-occurrence statistics.\n",
    "\n",
    "- **FastText (Facebook Research)**  \n",
    "  - Works on subword units (helps with rare words).\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**  \n",
    "  - Context-aware embeddings, more powerful than Word2Vec.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Conclusion**\n",
    "- Word2Vec converts words into numerical vectors that capture meaning.\n",
    "- It works using **CBOW** and **Skip-Gram** models.\n",
    "- We implemented Word2Vec **from scratch in PyTorch** and **used gensim for convenience**.\n",
    "- While powerful, Word2Vec has some limitations, and newer models like BERT are now more commonly used in NLP.\n",
    "\n",
    "Would you like a specific part explained in more detail? ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
