{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Detailed Explanation of FastText and How to Use It in PyTorch & Gensim**\n",
    "---\n",
    "\n",
    "## **1. What is FastText?**\n",
    "FastText is a **word embedding technique** developed by **Facebook's AI Research (FAIR)** that improves upon Word2Vec by considering **subword information**. It represents words **as a combination of character n-grams** instead of treating them as atomic entities.\n",
    "\n",
    "Unlike **Word2Vec**, which learns embeddings for entire words, **FastText learns embeddings for word fragments**, making it better at handling:\n",
    "- **Rare words** (since it uses subwords to create word vectors).\n",
    "- **Out-of-vocabulary (OOV) words** (it can generate embeddings for unseen words using subword components).\n",
    "- **Morphologically rich languages** (where words have many variations due to prefixes/suffixes).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How FastText Works**\n",
    "FastText builds word representations using **subword n-grams** (default is **3 to 6 characters** long).  \n",
    "\n",
    "For example, for the word **\"fasttext\"**, the 3-gram representation is:\n",
    "```\n",
    "<fa, fas, ast, stt, tte, tex, ext, xt>, fasttext\n",
    "```\n",
    "- `< >` are added to mark word boundaries.\n",
    "- The model learns embeddings for these **subword fragments**.\n",
    "- The final word embedding is obtained by averaging its **subword embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. FastText vs Word2Vec**\n",
    "| Feature           | Word2Vec  | FastText |\n",
    "|------------------|----------|----------|\n",
    "| Word Representation | Entire word | Character n-grams |\n",
    "| Handles Rare Words | ‚ùå No | ‚úÖ Yes |\n",
    "| Handles OOV Words | ‚ùå No | ‚úÖ Yes |\n",
    "| Morphologically Rich Languages | ‚ùå Poor | ‚úÖ Good |\n",
    "| Computational Cost | üü¢ Lower | üî¥ Slightly Higher |\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Implementing FastText in PyTorch (Step-by-Step)**\n",
    "We will implement FastText using **PyTorch** by modifying Word2Vec to include **subword embeddings**.\n",
    "\n",
    "### **Step 1: Install Dependencies**\n",
    "```bash\n",
    "pip install torch torchvision nltk\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Preprocess Text Data**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text data\n",
    "text = \"The cat sat on the mat. The dog barked at the cat.\"\n",
    "\n",
    "# Tokenize the text\n",
    "nltk.download('punkt')\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = list(set(tokens))\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "# Convert words to indices\n",
    "data = [word2idx[word] for word in tokens]\n",
    "\n",
    "print(\"Vocabulary:\", word2idx)\n",
    "print(\"Encoded Data:\", data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Generate Subword N-grams**\n",
    "```python\n",
    "def generate_ngrams(word, n=3):\n",
    "    word = f\"<{word}>\"  # Add boundary markers\n",
    "    ngrams = [word[i:i+n] for i in range(len(word) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Example\n",
    "print(\"Subwords for 'cat':\", generate_ngrams(\"cat\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Generate Training Data (Skip-Gram Model)**\n",
    "```python\n",
    "def generate_skipgram_data(data, window_size=2):\n",
    "    pairs = []\n",
    "    for center in range(len(data)):\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context = center + w\n",
    "            if context >= 0 and context < len(data) and context != center:\n",
    "                pairs.append((data[center], data[context]))\n",
    "    return pairs\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_skipgram_data(data, window_size)\n",
    "\n",
    "print(\"Example training pairs (word index):\", training_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Build the FastText Model in PyTorch**\n",
    "```python\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, ngram_size=3):\n",
    "        super(FastText, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "    def forward(self, center_word):\n",
    "        embedded = self.embeddings(center_word)  # Get word embeddings\n",
    "        output = self.linear(embedded)  # Predict context words\n",
    "        return output\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize model\n",
    "model = FastText(vocab_size, embedding_dim)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Train the Model**\n",
    "```python\n",
    "# Loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Convert data to tensors\n",
    "training_data_tensors = [(torch.tensor(center, dtype=torch.long),\n",
    "                          torch.tensor(context, dtype=torch.long)) for center, context in training_data]\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in training_data_tensors:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center.unsqueeze(0))  # Forward pass\n",
    "        loss = loss_function(output, context.unsqueeze(0))  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Extract Word Embeddings**\n",
    "```python\n",
    "word_vectors = model.embeddings.weight.data\n",
    "\n",
    "# Print embeddings for some words\n",
    "for word in [\"cat\", \"dog\", \"mat\"]:\n",
    "    print(f\"Embedding for '{word}': {word_vectors[word2idx[word]]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. FastText in Gensim (Easier Alternative)**\n",
    "Instead of training from scratch in PyTorch, we can use **Gensim**, which provides a pre-trained FastText model.\n",
    "\n",
    "```bash\n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "### **Train FastText using Gensim**\n",
    "```python\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\"The cat sat on the mat.\", \"The dog barked at the cat.\"]\n",
    "tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]\n",
    "\n",
    "# Train FastText model\n",
    "ft_model = FastText(sentences=tokenized_corpus, vector_size=10, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Get word vectors\n",
    "print(ft_model.wv[\"cat\"])  # Get vector for 'cat'\n",
    "print(ft_model.wv.most_similar(\"cat\"))  # Find similar words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Advantages of FastText**\n",
    "‚úÖ **Handles Rare and OOV Words**  \n",
    "   - Uses subwords to predict vectors even for unseen words.\n",
    "\n",
    "‚úÖ **Works Well in Morphologically Rich Languages**  \n",
    "   - E.g., German, Russian, and Turkish, where words change due to inflection.\n",
    "\n",
    "‚úÖ **Improves Accuracy in Text Classification**  \n",
    "   - Useful for tasks like sentiment analysis, document classification, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Disadvantages of FastText**\n",
    "üö´ **Higher Computational Cost**  \n",
    "   - Since it computes embeddings for subwords, training is slower than Word2Vec.\n",
    "\n",
    "üö´ **More Complex to Implement from Scratch**  \n",
    "   - Requires handling subwords efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Alternatives to FastText**\n",
    "- **Word2Vec**: Works well but doesn‚Äôt handle unseen words.\n",
    "- **GloVe**: Uses matrix factorization, effective for pre-trained embeddings.\n",
    "- **BERT**: Contextualized embeddings for deep NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Conclusion**\n",
    "- **FastText improves on Word2Vec by using subword n-grams.**\n",
    "- **It is robust against rare words and OOV words.**\n",
    "- **We implemented FastText from scratch in PyTorch and used Gensim for ease.**\n",
    "\n",
    "Would you like additional explanations on a specific part? üòä"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
